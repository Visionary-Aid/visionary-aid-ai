{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["# 1. Import & Mount Drive\n","from google.colab import drive\n","import os, json, zipfile, requests, random\n","import torch, torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","import numpy as np\n","from collections import defaultdict\n","from sklearn.metrics import classification_report\n","from torch.cuda.amp import autocast, GradScaler\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# 2. Download COCO 2017 val images and annotations\n","data_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n","annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n","\n","def download_and_extract(url, output_dir):\n","    filename = url.split(\"/\")[-1]\n","    output_path = os.path.join(output_dir, filename)\n","\n","    if not os.path.exists(output_path):\n","        response = requests.get(url, stream=True)\n","        total_size = int(response.headers.get(\"content-length\", 0))\n","        with open(output_path, \"wb\") as file, tqdm(desc=filename, total=total_size, unit=\"B\", unit_scale=True) as pb:\n","            for data in response.iter_content(chunk_size=1024):\n","                file.write(data)\n","                pb.update(len(data))\n","\n","    with zipfile.ZipFile(output_path, \"r\") as zip_ref:\n","        zip_ref.extractall(output_dir)\n","\n","download_and_extract(data_url, \"/content/\")\n","download_and_extract(annotations_url, \"/content/\")\n","\n","# 3. Load annotations\n","with open(\"/content/annotations/instances_val2017.json\", \"r\") as f:\n","    coco_data = json.load(f)\n","\n","# 4. Prepare all 80 categories\n","category_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in coco_data[\"categories\"]}\n","name_to_index = {name: idx for idx, name in enumerate(category_id_to_name.values())}\n","selected_categories = list(name_to_index.keys())\n","selected_cat_ids = list(category_id_to_name.keys())\n","\n","# 5. Filter and link images with single label\n","image_to_labels = defaultdict(set)\n","for ann in coco_data[\"annotations\"]:\n","    if ann[\"category_id\"] in selected_cat_ids:\n","        image_to_labels[ann[\"image_id\"].__int__()].add(ann[\"category_id\"])\n","\n","filtered_images = [img for img in coco_data[\"images\"] if len(image_to_labels[img[\"id\"]]) == 1]\n","random.shuffle(filtered_images)\n","filtered_images = filtered_images[:200]  # Reduced dataset size\n","\n","# 6. Split dataset\n","train_size = int(0.7 * len(filtered_images))\n","val_size = int(0.2 * len(filtered_images))\n","train_images = filtered_images[:train_size]\n","val_images = filtered_images[train_size:train_size+val_size]\n","test_images = filtered_images[train_size+val_size:]\n","\n","# 7. Improved Transforms with additional augmentations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(20),  # Reduced rotation angle\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# 8. Custom Dataset\n","class COCODataset(Dataset):\n","    def __init__(self, images, root_dir, transform):\n","        self.images = images\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_data = self.images[idx]\n","        img_path = os.path.join(self.root_dir, img_data[\"file_name\"])\n","        image = Image.open(img_path).convert(\"RGB\")\n","        image_id = img_data[\"id\"]\n","\n","        label_id = list(image_to_labels[image_id])[0]\n","        label_name = category_id_to_name[label_id]\n","        label = name_to_index[label_name]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","# 9. Data Loaders\n","train_loader = DataLoader(COCODataset(train_images, \"/content/val2017\", transform), batch_size=16, shuffle=True, num_workers=4)\n","val_loader = DataLoader(COCODataset(val_images, \"/content/val2017\", transform), batch_size=16, num_workers=4)\n","test_loader = DataLoader(COCODataset(test_images, \"/content/val2017\", transform), batch_size=16, num_workers=4)\n","\n","# 10. Use EfficientNet-B4 (smaller model for faster training)\n","model = models.efficientnet_b4(pretrained=True)\n","model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(selected_categories))\n","\n","# 11. Setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=1e-5)  # Lower learning rate for stability\n","\n","# 12. Training with Mixed Precision\n","scaler = GradScaler()\n","\n","def train_model_with_amp(model, train_loader, val_loader, epochs=5):  # Reduced epochs to 5\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            # Forward pass with mixed precision\n","            with autocast():\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            running_loss += loss.item()\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                val_loss += criterion(outputs, labels).item()\n","\n","        print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n","\n","# Training the model\n","train_model_with_amp(model, train_loader, val_loader, epochs=5)  # Reduced epochs to 5\n","\n","# 13. Evaluation\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    unique_labels = sorted(set(all_labels))\n","    used_class_names = [selected_categories[i] for i in unique_labels]\n","\n","    print(\"\\nDetailed Classification Report:\\n\")\n","    print(classification_report(all_labels, all_preds, labels=unique_labels, target_names=used_class_names, digits=4))\n","\n","evaluate_model(model, test_loader)\n","\n","# 14. Save model\n","model_path = \"/content/drive/MyDrive/efficientnet_coco_final.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")\n"],"metadata":{"id":"OnAjC1BZqIvW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745238407449,"user_tz":-120,"elapsed":2400716,"user":{"displayName":"Ahmed Ashraf","userId":"16903420960414342842"}},"outputId":"7f1f226b-ed16-44d3-8552-14739c8598ea"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n","100%|██████████| 74.5M/74.5M [00:00<00:00, 201MB/s]\n","<ipython-input-4-3b32033d81f3>:120: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","<ipython-input-4-3b32033d81f3>:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 4.3828, Val Loss: 4.3764\n","Epoch 2, Train Loss: 4.3885, Val Loss: 4.3856\n","Epoch 3, Train Loss: 4.3781, Val Loss: 4.3705\n","Epoch 4, Train Loss: 4.3694, Val Loss: 4.3691\n","Epoch 5, Train Loss: 4.3597, Val Loss: 4.3589\n","\n","Detailed Classification Report:\n","\n","               precision    recall  f1-score   support\n","\n","     airplane     0.0000    0.0000    0.0000       1.0\n","        train     0.0000    0.0000    0.0000       2.0\n","         boat     0.0000    0.0000    0.0000       1.0\n","traffic light     0.0000    0.0000    0.0000       1.0\n"," fire hydrant     0.0000    0.0000    0.0000       1.0\n","         bird     0.0000    0.0000    0.0000       1.0\n","        horse     0.0000    0.0000    0.0000       1.0\n","     elephant     0.0000    0.0000    0.0000       2.0\n","         bear     0.0000    0.0000    0.0000       1.0\n","        zebra     0.0000    0.0000    0.0000       2.0\n","     suitcase     0.0000    0.0000    0.0000       1.0\n","       toilet     0.0000    0.0000    0.0000       1.0\n"," refrigerator     0.0000    0.0000    0.0000       2.0\n","        clock     0.0000    0.0000    0.0000       2.0\n","     scissors     0.0000    0.0000    0.0000       1.0\n","\n","    micro avg     0.0000    0.0000    0.0000      20.0\n","    macro avg     0.0000    0.0000    0.0000      20.0\n"," weighted avg     0.0000    0.0000    0.0000      20.0\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Model saved to /content/drive/MyDrive/efficientnet_coco_final.pth\n"]}]}]}